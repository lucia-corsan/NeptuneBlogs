{"cells":[{"cell_type":"markdown","source":["### **Hands-on Practice**: A/B testing to analyze user feedback"],"metadata":{"collapsed":false,"id":"78a043050ffbf107"},"id":"78a043050ffbf107"},{"cell_type":"code","outputs":[],"source":["def simulate_user_feedback(prompt):\n","    # Simulate 100 user ratings between 1 and 5\n","    ratings = np.random.randint(1, 6, size=100)\n","    # Calculate the average rating\n","    average_rating = np.mean(ratings)\n","    return average_rating"],"metadata":{"id":"892f74719d817fca"},"id":"892f74719d817fca","execution_count":null},{"cell_type":"code","outputs":[],"source":["# Reinitialize Neptune for A/B testing\n","run = neptune.init(\n","    project='your_project_name',\n","    api_token='your_api_token'\n",")\n","\n","# Define A/B testing prompts\n","ab_prompts = {\n","    'A': \"How can I assist you with your order today?\",\n","    'B': \"Please tell me about any issues you’re experiencing with your order, and I’ll help you resolve them.\"\n","}\n","\n","for key, prompt in ab_prompts.items():\n","    satisfaction_score = simulate_user_feedback(prompt)\n","\n","    # Log metrics to Neptune\n","    run[f'A_B_testing/{key}/satisfaction_score'].log(satisfaction_score)\n","\n","    print(f\"Prompt {key} - Satisfaction Score: {satisfaction_score}\")\n","\n","# Stop the Neptune run\n","run.stop()\n"],"metadata":{"id":"b2eb71fd202ca54a"},"id":"b2eb71fd202ca54a","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3 (ipykernel)"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}